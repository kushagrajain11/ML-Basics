{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18741, 58)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open('abc.txt', 'r').read().lower()\n",
    "characters = list(set(data))\n",
    "len(data), len(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'\\t': 51,\n",
       "  '\\n': 32,\n",
       "  ' ': 38,\n",
       "  '!': 18,\n",
       "  '\"': 44,\n",
       "  '#': 55,\n",
       "  '$': 34,\n",
       "  '%': 40,\n",
       "  \"'\": 2,\n",
       "  '(': 1,\n",
       "  ')': 15,\n",
       "  '*': 45,\n",
       "  ',': 4,\n",
       "  '-': 11,\n",
       "  '.': 10,\n",
       "  '/': 56,\n",
       "  '0': 54,\n",
       "  '1': 37,\n",
       "  '2': 33,\n",
       "  '3': 20,\n",
       "  '4': 6,\n",
       "  '5': 28,\n",
       "  '6': 57,\n",
       "  '7': 24,\n",
       "  '8': 30,\n",
       "  '9': 14,\n",
       "  ':': 53,\n",
       "  ';': 42,\n",
       "  '=': 12,\n",
       "  '>': 22,\n",
       "  '?': 39,\n",
       "  '@': 21,\n",
       "  'a': 48,\n",
       "  'b': 19,\n",
       "  'c': 36,\n",
       "  'd': 3,\n",
       "  'e': 31,\n",
       "  'f': 26,\n",
       "  'g': 35,\n",
       "  'h': 7,\n",
       "  'i': 25,\n",
       "  'j': 5,\n",
       "  'k': 17,\n",
       "  'l': 47,\n",
       "  'm': 49,\n",
       "  'n': 8,\n",
       "  'o': 9,\n",
       "  'p': 41,\n",
       "  'q': 29,\n",
       "  'r': 16,\n",
       "  's': 50,\n",
       "  't': 52,\n",
       "  'u': 13,\n",
       "  'v': 27,\n",
       "  'w': 43,\n",
       "  'x': 0,\n",
       "  'y': 23,\n",
       "  'z': 46},\n",
       " {0: 'x',\n",
       "  1: '(',\n",
       "  2: \"'\",\n",
       "  3: 'd',\n",
       "  4: ',',\n",
       "  5: 'j',\n",
       "  6: '4',\n",
       "  7: 'h',\n",
       "  8: 'n',\n",
       "  9: 'o',\n",
       "  10: '.',\n",
       "  11: '-',\n",
       "  12: '=',\n",
       "  13: 'u',\n",
       "  14: '9',\n",
       "  15: ')',\n",
       "  16: 'r',\n",
       "  17: 'k',\n",
       "  18: '!',\n",
       "  19: 'b',\n",
       "  20: '3',\n",
       "  21: '@',\n",
       "  22: '>',\n",
       "  23: 'y',\n",
       "  24: '7',\n",
       "  25: 'i',\n",
       "  26: 'f',\n",
       "  27: 'v',\n",
       "  28: '5',\n",
       "  29: 'q',\n",
       "  30: '8',\n",
       "  31: 'e',\n",
       "  32: '\\n',\n",
       "  33: '2',\n",
       "  34: '$',\n",
       "  35: 'g',\n",
       "  36: 'c',\n",
       "  37: '1',\n",
       "  38: ' ',\n",
       "  39: '?',\n",
       "  40: '%',\n",
       "  41: 'p',\n",
       "  42: ';',\n",
       "  43: 'w',\n",
       "  44: '\"',\n",
       "  45: '*',\n",
       "  46: 'z',\n",
       "  47: 'l',\n",
       "  48: 'a',\n",
       "  49: 'm',\n",
       "  50: 's',\n",
       "  51: '\\t',\n",
       "  52: 't',\n",
       "  53: ':',\n",
       "  54: '0',\n",
       "  55: '#',\n",
       "  56: '/',\n",
       "  57: '6'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_index = {ch:i for i,ch in enumerate(characters)}\n",
    "index_to_char = {i:ch for i,ch in enumerate(characters)}\n",
    "\n",
    "char_to_index, index_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_units = 100\n",
    "learning_rate = 0.1\n",
    "length_seq = 20\n",
    "vocab_size = len(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Wxh = np.random.randn(hidden_units, vocab_size)* 0.01\n",
    "Whh = np.random.randn(hidden_units, hidden_units)* 0.01\n",
    "Why = np.random.randn(vocab_size, hidden_units)* 0.01\n",
    "bh = np.zeros((hidden_units, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = np.zeros((vocab_size, 1))\n",
    "inp[char_to_index['a']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_char(current_char, h_prev):\n",
    "    current_input = np.zeros((vocab_size, 1))\n",
    "    current_input[char_to_index[current_char]] = 1\n",
    "    hidden_output = np.tanh(bh + np.dot(Wxh, current_input) + np.dot(Whh, h_prev))\n",
    "    output = np.dot(Why, hidden_output) + by\n",
    "    prob = np.exp(output)/np.sum(np.exp(output))\n",
    "    max_index = np.argmax(prob)\n",
    "    output_char = index_to_char[max_index]\n",
    "    return output_char, hidden_output\n",
    "                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputs, targets, h_prev):\n",
    "    #h_prev = np.zeros((hidden_units, 1))\n",
    "    xs, hs, os, ps = {},{},{},{}\n",
    "    hs[-1] = np.copy(h_prev)\n",
    "    loss = 0\n",
    "    for i in range(len(inputs)):\n",
    "        xs[i] = np.zeros((vocab_size, 1))\n",
    "        xs[i][char_to_index[inputs[i]]] = 1\n",
    "        hs[i] = np.tanh(bh + np.dot(Wxh, xs[i]) + np.dot(Whh, hs[i - 1]))\n",
    "        os[i] = np.dot(Why, hs[i]) + by\n",
    "        ps[i] = np.exp(os[i])/np.sum(np.exp(os[i]))\n",
    "        loss += -np.log(ps[i][char_to_index[targets[i]], 0]) # TODO\n",
    "    # backward pass: compute gradients going backwards    \n",
    "    #initalize vectors for gradient values for each set of weights \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[char_to_index[targets[t]]] -= 1 # backprop into y  \n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "        dbh += dhraw #derivative of hidden bias\n",
    "        dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "        dhnext = np.dot(Whh.T, dhraw) \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sentence(seed, n, h_prev):\n",
    "    character = seed\n",
    "    sentence = \"\" + seed\n",
    "    for i in range(n):\n",
    "        character, h_prev = next_char(character, h_prev)\n",
    "        sentence += character\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 81.208856\n",
      "iter 1000, loss: 66.875676\n",
      "iter 2000, loss: 56.856521\n",
      "iter 3000, loss: 52.874632\n",
      "iter 4000, loss: 50.350039\n",
      "iter 5000, loss: 48.650209\n",
      "iter 6000, loss: 47.440236\n",
      "iter 7000, loss: 46.236625\n",
      "iter 8000, loss: 45.408042\n",
      "iter 9000, loss: 45.642357\n",
      "iter 10000, loss: 44.894843\n",
      "iter 11000, loss: 44.180518\n",
      "iter 12000, loss: 43.395132\n",
      "iter 13000, loss: 42.638883\n",
      "iter 14000, loss: 41.980876\n",
      "iter 15000, loss: 41.417475\n",
      "iter 16000, loss: 40.908004\n",
      "iter 17000, loss: 40.651144\n",
      "iter 18000, loss: 40.291458\n",
      "iter 19000, loss: 39.975696\n",
      "iter 20000, loss: 39.823343\n",
      "iter 21000, loss: 39.472068\n",
      "iter 22000, loss: 39.113851\n",
      "iter 23000, loss: 38.788883\n",
      "iter 24000, loss: 38.693271\n",
      "iter 25000, loss: 38.332453\n",
      "iter 26000, loss: 38.019409\n",
      "iter 27000, loss: 37.697628\n",
      "iter 28000, loss: 37.453395\n",
      "iter 29000, loss: 37.187348\n",
      "iter 30000, loss: 36.979780\n",
      "iter 31000, loss: 36.844359\n",
      "iter 32000, loss: 36.761309\n",
      "iter 33000, loss: 36.642241\n",
      "iter 34000, loss: 36.604001\n",
      "iter 35000, loss: 36.583973\n",
      "iter 36000, loss: 36.260094\n",
      "iter 37000, loss: 36.055347\n",
      "iter 38000, loss: 35.896819\n",
      "iter 39000, loss: 35.725059\n",
      "iter 40000, loss: 35.419783\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*length_seq # loss at iteration 0           \n",
    "while n<=1000*40:\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    # check \"How to feed the loss function to see how this part works\n",
    "    if p+length_seq+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_units,1)) # reset RNN memory                                                                                                                                      \n",
    "        p = 0 # go from start of data                                                                                                                                                             \n",
    "    \n",
    "    inputs = [ch for ch in data[p:p+length_seq]]\n",
    "    targets = [ch for ch in data[p+1:p+length_seq+1]]\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = train(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    # sample from the model now and then                                                                                                                                                        \n",
    "    if n % 1000 == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "        #sample(hprev, inputs[0], 200)\n",
    "\n",
    "    # perform parameter update with Adagrad                                                                                                                                                     \n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        current_learning = learning_rate/np.sqrt(mem + 1e-8)\n",
    "        param += -current_learning * dparam  # adagrad update                                                                                                                   \n",
    "\n",
    "    p += length_seq # move data pointer                                                                                                                                                         \n",
    "    n += 1 # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an was sin the ing the fill the tion we his simes a\n"
     ]
    }
   ],
   "source": [
    "generate_sentence('a', 50, hprev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
